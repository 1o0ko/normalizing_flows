{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Normalizing Flows - VAE",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1o0ko/normalizing_flows/blob/master/Normalizing_Flows_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8OsY4hSNbfri",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Variational Inference with Normalizing Flows\n",
        "\n",
        "## Types of Generative Models\n",
        "\n",
        "\n",
        "1.   **Generative Aversarial Networks**:  Generator and Discriminator, discriminator learns to distinguish the real data from the fake samples that are produced by the generator model. \n",
        "2.   ** Variational Autoencoders**: VAE inexplicitly optimizes the log-likelyhood of the data by maximizing the evidence lower bound (ELBO)\n",
        "3. ** Flow-based** generative models: are constructed by a sequence of invertible transformations. Unlike GANs and VAEs the model explicitly learns the true data distribution $p(\\mathbf x)$ and the loss function is simply the negative log-likelyhood.\n",
        "\n",
        "\n",
        "Stolen from From [Lilian Weng's](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#jacobian-matrix-and-determinant) blog:\n",
        "\n",
        "\n",
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/three-generative-models.png)"
      ]
    },
    {
      "metadata": {
        "id": "FmtkFxoMbht-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGjth91rT2ad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip3 install torch torchvision\n",
        "! pip3 install pillow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R75oNahj5Z3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxiJQitlLxBR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Normalizing Flows\n",
        "\n",
        "\n",
        "The basic rule for transformation of densities consideres an invertible, smooth mapping $f: \\mathbb{R}^D \\rightarrow  \\mathbb{R}^D$ with an inverse $f^{-1}=g$, such that  $ g \\circ f (\\textbf{z}) = \\textbf{z}$.  If we use this mapping to transform a random variable $\\mathbf{z}$ with distribution $q(\\mathbf{z})$, then the resulting random variable $\\mathbf{z}^\\prime = f(\\mathbf{z}$) has a distribution:\n",
        "\n",
        "$$\n",
        "q(\\mathbf{z}^{\\prime}) = q(\\mathbf{z}) \n",
        "  \\left| \n",
        "    \\det \\frac{\\partial f^{-1}}{\\partial \\mathbf{z}^\\prime}\n",
        "  \\right| = \n",
        "   q(\\mathbf{z}) \n",
        "  \\left| \n",
        "    \\det \\frac{\\partial f}{\\partial \\mathbf{z}}\n",
        "  \\right|^{-1},\n",
        "$$\n",
        "where the last equality can be obtained by applying [the inverse function theorem](https://en.wikipedia.org/wiki/Inverse_function_theorem) and taking advantage of the property of Jacobians of intertible functions.\n",
        "\n",
        "The density $q_K(\\mathbf z)$ obtained by successively transforming a random variable $\\mathbf z_0$ with distribution\n",
        "$q_0$ through a chain of $K$ transformations $f_k$ is:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbf z_K &= f_K \\circ \\ldots \\circ f_1( \\mathbf z_0), \\\\\\\n",
        "  \\ln q_K (\\mathbf z_K) &= \\ln q_0(\\mathbf z_0) - \\sum_{k=1}^{K} \\ln \\det \\frac{\\partial f_k}{\\partial \\mathbf{z}_{k-1}}.\n",
        "\\end{align}\n",
        "\n",
        "The formalism of normalizing flows now gives us a systematic\n",
        "way of specifying the approximate posterior distributions\n",
        "$q(\\mathbf z| \\mathbf x)$ required for variational inference. With an\n",
        "appropriate choice of transformations $f_K$, we can initially\n",
        "use simple factorized distributions such as an independent\n",
        "Gaussian, and apply normalizing flows of different lengths\n",
        "to obtain increasingly complex and multi-modal distributions.\n",
        "\n",
        "\n",
        "\n",
        "From [Lilian Weng's  blog](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html#jacobian-matrix-and-determinant) (she uses $p_i$ instead of $q_i$):\n",
        "![alt text](https://lilianweng.github.io/lil-log/assets/images/normalizing-flow.png)\n",
        "\n",
        "###  Remark:\n",
        "If  $\\mathbf{p}$ is a point in $\\mathbb{R}^D$ and $f$ is differentiable at $\\mathbf{p}$, then its derivative is given by $J_f(\\mathbf{p})$. In this case, the linear map described by $J_f(\\mathbf{p})$ is the best linear approximation of $f$ near the point $\\mathbf{p}$, in the sense that\n",
        "\n",
        "$$\n",
        "\\mathbf f(\\mathbf x) = \\mathbf f(\\mathbf p) + \\mathbf J_{\\mathbf f}(\\mathbf p)(\\mathbf x - \\mathbf p) + o(\\|\\mathbf x - \\mathbf p\\|),\n",
        "$$\n",
        "\n",
        "where $\\mathbf x$ is close to $\\mathbf p$ and where $o$ is the little o-notation.\n",
        "\n",
        "Since, we can percieve the Jacobian of $f: \\mathbb{R}^D \\rightarrow  \\mathbb{R}^D$ as locally linear map, we can describe the space distortions using the determinant: geometrically the absolute value of the Jacobian determinant gives the magnification/scalling factor when we transform an area or volume. It intuitevely make sense, that if function changes the volume by $a$ it's inverse should change the volme by $\\frac{1}{a}$. "
      ]
    },
    {
      "metadata": {
        "id": "0uJ_n0H0_uqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Abstract Flow"
      ]
    },
    {
      "metadata": {
        "id": "P4eU3GF1NtVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Tuple\n",
        "from torch import Tensor\n",
        "\n",
        "class Flow(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, z, parameters: Tuple[Tensor]) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def log_det_jacobian(self, z, parameters: Tuple[Tensor]):\n",
        "        pass\n",
        "      \n",
        "    @abstractmethod\n",
        "    def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "        '''\n",
        "        Method used to unpack the hidden layer to parameters of the flow\n",
        "        \n",
        "        From section 4.2:\n",
        "        For amortized variational inference, we construct an inference model\n",
        "        using a deep neural network to build a mapping from the observations x\n",
        "        to the parameters of the initial density q0 = N(µ, σ) (µ∈R^D and σ∈R^D)\n",
        "        as well as the parameters of the flow λ.\n",
        "        '''\n",
        "        pass\n",
        "    \n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def dim(self) -> int:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QhpGnFeN3dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Planar Flow"
      ]
    },
    {
      "metadata": {
        "id": "ZodQxYmbNyb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def safe_log(z):\n",
        "    return torch.log(z + 1e-7)\n",
        "  \n",
        "def tanh(x):\n",
        "  return torch.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_prime(x):\n",
        "  return 1 - torch.tanh(x)**2\n",
        "\n",
        "\n",
        "class PlanarFlow(nn.Module, Flow):\n",
        "  def __init__(self, dim = 2, h = tanh, h_prime= tanh_prime):\n",
        "      super().__init__()\n",
        "      '''\n",
        "      f(z) = z + u h(w^T @ z + b)\n",
        "      \n",
        "      \n",
        "      The flow defined by the transformation above modifies the\n",
        "      initial density q_0 by applying a series of contractions and\n",
        "      expansions in the direction perpendicular to the hyperplane\n",
        "      w^T z+b = 0, hence we refer to these maps as planar flows.\n",
        "      '''\n",
        "\n",
        "\n",
        "      # h(·) is a smooth element-wise non-linearity\n",
        "      self.h = h\n",
        "      self.h_prime = h_prime\n",
        "      self.d = dim\n",
        "       \n",
        "\n",
        "  def forward(self, z, parameters: Tuple[Tensor]) -> Tensor:\n",
        "      '''\n",
        "      f(z) = z + u h(w^T @ z + b)\n",
        "      '''\n",
        "      u, w, b = parameters\n",
        "      \n",
        "      z = z + self.h(F.linear(z, w, b)) @ u\n",
        "      return z\n",
        "  \n",
        "  def log_det_jacobian(self, z, parameters: Tuple[Tensor]):\n",
        "      '''\n",
        "      ψ(z) = h'(w^T @ z + b)w\n",
        "      |det @f/@z | = |1 + u^T ψ(z)|\n",
        "      \n",
        "      '''\n",
        "      u, w, b = parameters\n",
        "      \n",
        "      \n",
        "      psi = self.h_prime(F.linear(z, w, b)) @ w\n",
        "      det_jacobian = torch.abs(1 + F.linear(psi, u))\n",
        "      return safe_log(det_jacobian)\n",
        "    \n",
        "  def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "      ''' unpacks the free parameters\n",
        "      λ = {w ∈ R^D, u ∈ R^D, b ∈ R} are free parameters\n",
        "      '''\n",
        "      w, u = parameters[:, :-1].chunk(2, dim=1)\n",
        "      b = parameters[:, -1].view(-1, 1)\n",
        "      \n",
        "      return (w, u, b)\n",
        "\n",
        "  @property\n",
        "  def dim(self):\n",
        "      return 2 * self.d + 1\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5O-SqUAON2wR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Radial Flow"
      ]
    },
    {
      "metadata": {
        "id": "tKorcChlNkKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RadialFlow(nn.Module, Flow):\n",
        "    def __init__(self, dim: int, h = tanh, h_prime= tanh_prime):\n",
        "      super().__init__()\n",
        "      '''\n",
        "      f(z) = z + βh(α, r)(z − z0),\n",
        "      \n",
        "      \n",
        "       It applies radial contractions and expansions\n",
        "       around the reference point and are thus referred to as\n",
        "       radial flows\n",
        "      '''\n",
        "      \n",
        "      # λ = {z_0 ∈ R^D, α ∈ R+, β ∈ R}\n",
        "\n",
        "\n",
        "      # h(·) is a smooth element-wise non-linearity\n",
        "      self.h = h\n",
        "      self.h_prime = h_prime\n",
        "      \n",
        "    \n",
        "    \n",
        "    def forward(self, z, parameters: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    def log_det_jacobian(self, z, parameters: Tensor):\n",
        "        pass\n",
        "      \n",
        "    def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQyEB9dMN83u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalizing Flows"
      ]
    },
    {
      "metadata": {
        "id": "fgVIWd-wNnZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NormalizingFlow(nn.Module):\n",
        "    def __init__(self, K: int, flow_class, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.flows = nn.Sequential(*(\n",
        "            flow_class(*args, **kwargs) for _ in range(K)\n",
        "        ))\n",
        "      \n",
        "    def forward(self, z, lambdas: List[Tensor]):\n",
        "      log_abs_det_jacobians = []\n",
        "\n",
        "      for flow, parameters in zip(self.flows, lambdas):\n",
        "          log_abs_det_jacobians.append(flow.log_det_jacobian(z, parameters))\n",
        "          z = flow(z, parameters)\n",
        "          \n",
        "      return z, sum(log_abs_det_jacobians)\n",
        "    \n",
        "    def unpack(self, params: Tensor) -> List[Tensor]:\n",
        "      flow_params = []\n",
        "      start, end = 0, 0\n",
        "      \n",
        "      for flow in self.flows:\n",
        "        start, end = end, end + flow.dim\n",
        "        flow_params.append(flow.unpack(params[:, start:end]))\n",
        "\n",
        "      return flow_params\n",
        "    \n",
        "    @property\n",
        "    def dims(self):\n",
        "      return sum(flow.dim for flow in self.flows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJV3bqpfG4wD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def h(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def h_prime(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def f(z, w, u, b):\n",
        "    return z + np.dot(h(np.dot(z, w) + b).reshape(-1,1), u.reshape(1,-1))\n",
        "  \n",
        "  \n",
        "def plot_flow():\n",
        "  plt.figure(figsize=[10, 14])\n",
        "\n",
        "  id_figure = 1\n",
        "  for i in np.arange(5):\n",
        "      for j in np.arange(5):\n",
        "          #represent w and u in polar coordinate system\n",
        "          theta_w = 0\n",
        "          rho_w = 5\n",
        "          theta_u = np.pi / 8 * i\n",
        "          rho_u = j / 4.0\n",
        "          \n",
        "          w = np.array([np.cos(theta_w), np.sin(theta_w)]) * rho_w\n",
        "          u = np.array([np.cos(theta_u), np.sin(theta_u)]) * rho_u\n",
        "          b = 0\n",
        "\n",
        "          grid_use = np.meshgrid(np.arange(-1,1,0.001), np.arange(-1,1,0.001))\n",
        "          z = np.concatenate([grid_use[0].reshape(-1,1), grid_use[1].reshape(-1,1)], axis=1)\n",
        "          z = np.random.normal(size=(int(1e6),2))\n",
        "          z_new = f(z, w, u, b)\n",
        "\n",
        "          heatmap, xedges, yedges = np.histogram2d(\n",
        "              z_new[:,0], z_new[:,1], bins=50, range=[[-3,3],[-3,3]])\n",
        "\n",
        "          extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
        "\n",
        "          plt.subplot(5,5,id_figure)\n",
        "          plt.imshow(heatmap, extent=extent, cmap='viridis')\n",
        "          plt.title(\"u=(%.1f,%.1f)\"%(u[0],u[1]) + \"\\n\" +\n",
        "                    \"w=(%d,%d)\"%(w[0],w[1]) + \", \" + \"b=%d\"%b)\n",
        "          id_figure += 1\n",
        "\n",
        "          plt.xlim([-3,3])\n",
        "          plt.ylim([-3,3])\n",
        "        \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bE9jgLd9jR9V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The effect of planar and radial flows on the Gaussian and uniform distributions. The figure comes from the original paper.\n",
        "![alt text](http://akosiorek.github.io/resources/simple_flows.png)"
      ]
    },
    {
      "metadata": {
        "id": "C1U1coegJJHL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Distributions"
      ]
    },
    {
      "metadata": {
        "id": "x3C0y_fzJGhu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "from numbers import Number\n",
        "from itertools import accumulate\n",
        "from bisect import bisect_right\n",
        "\n",
        "\n",
        "  \n",
        "def p_1(z, shift = 2, scale_1 = 0.4 , scale_2 = 0.6):\n",
        "    '''\n",
        "    Unnormalized fancy 2D density number 1\n",
        "    '''\n",
        "\n",
        "    z1, z2 = torch.chunk(z, chunks=2, dim=1)\n",
        "    norm = torch.sqrt(z1 ** 2 + z2 ** 2)\n",
        "\n",
        "    exp1 = torch.exp(-0.5 * ((z1 - shift) / scale_2) ** 2)\n",
        "    exp2 = torch.exp(-0.5 * ((z1 + shift) / scale_2) ** 2)\n",
        "    u = 0.5 * ((norm - shift) / scale_1) ** 2 - safe_log(exp1 + exp2)\n",
        "\n",
        "    return torch.exp(-u)\n",
        "\n",
        "\n",
        "def sum_probs(point_1, point_2):\n",
        "  z_1, x_1, y_1 = point_1\n",
        "  z_2, x_2, y_2 = point_2\n",
        "  return z_1 + z_2, x_2, y_2\n",
        "\n",
        "\n",
        "def find_le(a, x):\n",
        "    'Find rightmost value less than or equal to x'\n",
        "    i = bisect_right(a, (x, ))\n",
        "    if i:\n",
        "        return a[i-1]\n",
        "    raise ValueError\n",
        "\n",
        "def sample(points):\n",
        "  p, x, y = find_le(points, random.random())\n",
        "  return (x, y)\n",
        "\n",
        "\n",
        "class EmpiricalSampler:\n",
        "  def __init__(self,  \n",
        "               density,\n",
        "               n_points: int = 600, \n",
        "               limits: Tuple[float] = (-4, 4)):\n",
        "    '''\n",
        "    Wrapper class to sample from a close form bivariate-pdf\n",
        "    '''\n",
        "    self.density = density\n",
        "\n",
        "    x = np.linspace(*limits, n_points)\n",
        "    y = np.linspace(*limits, n_points)\n",
        "    x, y = np.meshgrid(x, y)\n",
        "    z = density(Tensor(np.c_[x, y])).data.numpy().reshape((n_points, n_points))\n",
        "    z = z / np.sum(z)\n",
        "    \n",
        "    points = zip(z.ravel(), x.ravel(), y.ravel())\n",
        "    points = list(accumulate(points, sum_probs))\n",
        "    self.points = points\n",
        "    \n",
        "  def sample(self, n: int):\n",
        "    return np.array([sample(self.points) for _ in range(n)])\n",
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(self, dim: int = 2):\n",
        "      self.d = 2\n",
        "      \n",
        "    def unpack(self, parameters: Tensor) -> Tuple[Tensor, ...]:\n",
        "      ''' takes hidden state and returns mu and sigma'''\n",
        "      mu, log_var = parameters.chunk(2, dim=1)\n",
        "      std = torch.exp(0.5*log_var)\n",
        "      \n",
        "      return mu, std \n",
        "    \n",
        "    def sample_with_log_prob(self, n, parameters: Tensor):\n",
        "      mu, std = parameters\n",
        "      \n",
        "      xs = mu + std * Gaussian.sample(n, self.d)\n",
        "\n",
        "      return xs, self.log_prob(xs, mu, std)\n",
        "    \n",
        "    @classmethod\n",
        "    def sample(cls, n: int, d: int, mean: float = 0, std:float = 1):\n",
        "        return torch.zeros(n, d).normal_(mean=mean, std=std)\n",
        "    \n",
        "    @classmethod\n",
        "    def log_prob(cls, value, mu, std):\n",
        "      '''\n",
        "      Log of density function of multivariate normal with diagonal \n",
        "      covariance matrix\n",
        "      '''\n",
        "      var = std ** 2\n",
        "      log_std = math.log(std) if isinstance(std, Number) else std.log()\n",
        "      return (-((value - mu) ** 2) / (2 * var) - log_std - math.log(math.sqrt(2 * math.pi) )).sum(1, True)\n",
        "\n",
        "    \n",
        "    @property\n",
        "    def dims(self):\n",
        "      ''' \n",
        "      params for mu and sigma\n",
        "      '''\n",
        "      return 2 * self.d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLmXEOcGBS3q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Flow-Based Free Energy Bound"
      ]
    },
    {
      "metadata": {
        "id": "3XO9pTUm5r1l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FreeEnergyBound(nn.Module):\n",
        "\n",
        "    def __init__(self, p_x):\n",
        "        super().__init__()\n",
        "        self.p_x = p_x\n",
        "        \n",
        "\n",
        "    def forward(self, log_q_0, z_k, log_jacobians, beta):\n",
        "        energy =  log_q_0 \\\n",
        "                - beta * safe_log(self.p_x.density(z_k)) \\\n",
        "                - log_jacobians\n",
        "        \n",
        "        return energy.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlAW9GM_GUnK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def scatter_points(points, directory, iteration, flow_length):\n",
        "\n",
        "    X_LIMS = (-4, 4)\n",
        "    Y_LIMS = (-4, 4)\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(points[:, 0], points[:, 1], alpha=0.7, s=25)\n",
        "    ax.set_xlim(*X_LIMS)\n",
        "    ax.set_ylim(*Y_LIMS)\n",
        "    ax.set_title(\n",
        "        \"Flow length: {}\\n Samples on iteration #{}\"\n",
        "        .format(flow_length, iteration)\n",
        "    )\n",
        "\n",
        "    fig.savefig(os.path.join(directory, \"flow_result_{}.png\".format(iteration)))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_density(distribution, directory):\n",
        "\n",
        "    X_LIMS = (-4, 4)\n",
        "    Y_LIMS = (-4, 4)\n",
        "\n",
        "    x1 = np.linspace(*X_LIMS, 300)\n",
        "    x2 = np.linspace(*Y_LIMS, 300)\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "    shape = x1.shape\n",
        "    x1 = x1.ravel()\n",
        "    x2 = x2.ravel()\n",
        "\n",
        "    z = np.c_[x1, x2]\n",
        "    z = torch.FloatTensor(z)\n",
        "    z = Variable(z)\n",
        "\n",
        "    density_values = distribution.density(z).data.numpy().reshape(shape)\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.imshow(density_values, extent=(*X_LIMS, *Y_LIMS), cmap=\"summer\")\n",
        "    ax.set_title(\"True density\")\n",
        "\n",
        "    fig.savefig(os.path.join(directory, \"density.png\"))\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swCIzq5SMo6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "class Maxout(nn.Module):\n",
        "    def __init__(self, pool_size: int = 4):\n",
        "        super().__init__()\n",
        "        self._pool_size = pool_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[-1] % self._pool_size == 0, \\\n",
        "            f\"Wrong input last dim size ({x.shape[-1]}) for Maxout({self._pool_size})\"\n",
        "        \n",
        "        m, i = x.view(*x.shape[:-1], x.shape[-1] // self._pool_size, self._pool_size).max(-1)\n",
        "        \n",
        "        return m\n",
        "      \n",
        "\n",
        "class InferenceNetwork(nn.Module):\n",
        "    def __init__(self, distribution, flows, sizes: List[Tuple[int]], pool_size: int = 4):\n",
        "      '''\n",
        "      Inference model using a deep neural network to build a mapping\n",
        "      from the observations x to the parameters.\n",
        "      \n",
        "      '''\n",
        "      super().__init__()\n",
        "      \n",
        "   \n",
        "      self.flows = flows\n",
        "      self.distribution = distribution\n",
        "      \n",
        "      layers = list(chain.from_iterable([\n",
        "          (nn.Linear(d_in, d_out*pool_size), Maxout(pool_size=pool_size)) for d_in, d_out in sizes\n",
        "      ]))\n",
        "      \n",
        "      d_in = sizes[-1][-1]\n",
        "      d_out = (self.flows.dims + self.distribution.dims) * pool_size\n",
        "      \n",
        "      layers.append(nn.Linear(d_in, d_out))\n",
        "      layers.append(Maxout(pool_size))\n",
        "      \n",
        "      \n",
        "      self.net = nn.Sequential(*layers)\n",
        "      self.net.apply(init_weights)\n",
        "      \n",
        "    \n",
        "    def forward(self, x):\n",
        "      parameters = self.net(x)\n",
        "      # unpack initial distribution parameters\n",
        "      start, end = 0, self.distribution.dims\n",
        "      dist_params = self.distribution.unpack(parameters[:, start:end])\n",
        "      \n",
        "      # unpack flow parameters\n",
        "      flow_params = self.flows.unpack(parameters[:, start:])\n",
        "      \n",
        "      return dist_params, flow_params\n",
        "    \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLFdE4bLw8q4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "9hN4WHBocAFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/results/'\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WjVetqBRHrJx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K = 2\n",
        "\n",
        "fancy_dist = EmpiricalSampler(p_1)\n",
        "\n",
        "q_0 = Gaussian(2)\n",
        "flow = NormalizingFlow(K=K, flow_class=PlanarFlow, dim=2)\n",
        "\n",
        "net = InferenceNetwork(q_0, flow, [(2, 100)])\n",
        "annealed_bound = FreeEnergyBound(p_x = fancy_dist)\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=1e-5, momentum=0.9)\n",
        "\n",
        "for iteration in range(1, 500000):\n",
        "    \n",
        "    # get samples from the true distribution\n",
        "    true_samples = Tensor(fancy_dist.sample(100))\n",
        "    \n",
        "    # use inference network to find the parameters of the\n",
        "\n",
        "    dist_params, flow_params = net(true_samples)\n",
        "    z_0, log_q_0 = q_0.sample_with_log_prob(100, dist_params)\n",
        "    z_k, log_jacobians = flow(z_0, flow_params)\n",
        "    loss = annealed_bound(log_q_0, z_k, log_jacobians, min(1, 0.01 + iteration/1000))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if iteration % 1000 == 0:\n",
        "      print(\"Loss on iteration {}: {}\".format(iteration , loss.data.item()))\n",
        "    \n",
        "    if iteration % 10000 == 0:\n",
        "        scatter_points(\n",
        "            z_k.data.numpy(),\n",
        "            directory='/content/results/',\n",
        "            iteration=iteration,\n",
        "            flow_length=K\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLr0ie6Z_igv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vatiational Autoencoders"
      ]
    },
    {
      "metadata": {
        "id": "82AL27KjxMd_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vanilla VAE"
      ]
    },
    {
      "metadata": {
        "id": "g3CO-FWqSUdH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    '''\n",
        "    On Mnist\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 feature_size: int = 784,\n",
        "                 hidden_size: int = 400,\n",
        "                 code_size: int = 20):\n",
        "        super(VAE, self).__init__()\n",
        "        self.code_size = code_size\n",
        "        self.feature_size = feature_size\n",
        " \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(feature_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, code_size * 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(code_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, feature_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std*eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.feature_size)\n",
        "        \n",
        "        mu, log_var = self.encoder(x).chunk(2, dim=1)\n",
        "\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        \n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        kl_div = kl_div / x.size(0)  # mean over batch\n",
        "        \n",
        "        return self.decoder(z), kl_div"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-S0lQXDsVhMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reconstruction_loss(recon_x, x):\n",
        "    # batch mean\n",
        "    return F.binary_cross_entropy(recon_x, x, reduction=\"sum\") / x.size(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YFYcafr0SZma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, test_loader,\n",
        "                 log_interval: int =10,\n",
        "                 batch_size: int =128):\n",
        "      self.model = model\n",
        "      self.train_loader = train_loader\n",
        "      self.test_loader = train_loader\n",
        "\n",
        "      self.log_interval = log_interval\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
        "            data = data.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            recon_batch, kl_div = self.model(data)\n",
        "            recon_loss = reconstruction_loss(recon_batch, data)\n",
        "            loss =  recon_loss + kl_div\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if batch_idx % self.log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss.item() / len(data)))\n",
        "                \n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tRecon: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    recon_loss.item() / len(data)))\n",
        "                \n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tKL: {:.6f}\\n'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    kl_div.item() / len(data)))\n",
        "                \n",
        "\n",
        "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "              epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "    def test(self, epoch, name, fixed_sample = None):\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (data, _) in enumerate(self.test_loader):\n",
        "                data = data.to(device)                \n",
        "                recon_batch, kl_div = self.model(data)\n",
        "                test_loss += (reconstruction_loss(recon_batch, data) + kl_div).item()\n",
        "                \n",
        "                if i == 0:\n",
        "                    n = min(data.size(0), 8)\n",
        "                    comparison = torch.cat([\n",
        "                        data[:n],\n",
        "                        recon_batch.view(self.batch_size, 1, 28, 28)[:n]\n",
        "                    ])\n",
        "                    \n",
        "                    save_image(comparison.cpu(),\n",
        "                             f'/content/results/reconstruction_{name}_{epoch}.png', nrow=n)\n",
        "              \n",
        "            if not fixed_sample is None:\n",
        "                fixed_sample = fixed_sample.to(device)\n",
        "                recon_sample, _ = self.model(fixed_sample)\n",
        "                recon_sample = recon_sample.view(8, 1, 28, 28)\n",
        "                \n",
        "                n = min(fixed_sample.size(0), 8)\n",
        "                comparison = torch.cat([\n",
        "                    fixed_sample[:n],\n",
        "                    recon_sample[:n]\n",
        "                ])\n",
        "                \n",
        "                save_image(comparison.cpu(),\n",
        "                           f'/content/results/fixed_reconstruction_{name}_{epoch}.png', nrow=n)   \n",
        "              \n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "        \n",
        "      \n",
        "    def run(self, num_epochs, name, fixed_sample = None):\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "          self.train(epoch)\n",
        "          self.test(epoch, name, fixed_sample)\n",
        "          with torch.no_grad():\n",
        "              sample = torch.randn(64, model.code_size).to(device)\n",
        "              sample = model.decoder(sample).cpu()\n",
        "              sample_name = f\"results/sample_{name}_{epoch}.png\"\n",
        "              save_image(sample.view(64, 1, 28, 28), sample_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnWkSbKZSxT9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdPoLDEtTDWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainer.run(10, 'VAE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "USdDkY_gS9eI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VAE(code_size=40)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model, \n",
        "    train_loader = train_loader,\n",
        "    test_loader = test_loader,\n",
        "    log_interval = 100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YeyDr4LiScZC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## VAE with Normalizing Flows"
      ]
    },
    {
      "metadata": {
        "id": "J2k4NFnx_mcF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE_NF(nn.Module):\n",
        "    '''\n",
        "    On Mnist\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 flows: NormalizingFlow,\n",
        "                 feature_size: int = 784,\n",
        "                 hidden_size: int = 400,\n",
        "                 code_size: int = 20):\n",
        "        super(VAE_NF, self).__init__()\n",
        "        self.flow = flow\n",
        "        self.code_size = code_size\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(feature_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, code_size * 2 + self.flow.dims)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(code_size, hidden_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(hidden_size, feature_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std*eps\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run inference network to get the posterior parameters\n",
        "        params = self.encoder(x.view(-1, 784))\n",
        "        \n",
        "        mu = params[:, :self.code_size]\n",
        "        log_var = params[:, self.code_size: self.code_size * 2]\n",
        "        flow_params = self.flow.unpack(params[:, self.code_size*2:])\n",
        "        \n",
        "        # Get samples from posterior\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        z_K, log_jacobians = self.flow(z, flow_params)\n",
        "        \n",
        "        # Push it through generative network\n",
        "        x_recon = self.decoder(z_K)\n",
        "        \n",
        "        # Calculate the loss\n",
        "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        kl_div = kl_div / x.size(0) - log_jacobians.mean()\n",
        "        \n",
        "        return x_recon, kl_div\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-P6b3ZGOAqHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "code_size = 40\n",
        "\n",
        "flow = NormalizingFlow(K=4, flow_class=PlanarFlow, dim=code_size)\n",
        "\n",
        "model = VAE_NF(flow, code_size=code_size)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model, \n",
        "    train_loader = train_loader,\n",
        "    test_loader = test_loader,\n",
        "    log_interval = 100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h5KQp9GYo3-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fixed_sample, _ = next(iter(test_loader))\n",
        "fixed_sample = fixed_sample[:8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80x_hplOPet2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainer.run(100, 'VAE_NF', fixed_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}